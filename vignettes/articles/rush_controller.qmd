---
title: "Rush Controller"
vignette: >
  %\VignetteIndexEntry{Rush Manager}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

```{r}
#| include: false
r = redux::hiredis()
r$FLUSHDB()
```


# Introduction

The `Rush` controller manages the network of workers.
The controller starts local and remote workers and monitors their status.
If a worker fails, the controller can detect the failure and restart the worker.
The controller can terminate the workers or kill them.
The controller can read log messages from the workers.

# Local Workers

We use the random search example again to demonstrate how the controller works.

```{r}
library(rush)

wl_random_search = function(rush) {
  repeat {
    # draw new task
    xs = list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15))

    # mark task as running
    key = rush$push_running_tasks(xss = list(xs))

    # evaluate task
    ys = list(y = branin(xs$x1, xs$x2))

    # push result
    rush$push_results(key, yss = list(ys))

    # stop optimization after 100 tasks
    if (rush$n_finished_tasks >= 100) break
  }
}


# Connection to the Redis database
config = redux::redis_config()

# Initialize rush controller
rush = rsh(
  network = "test-random-search",
  config = config)
```

## Start Workers

Workers can be started on the local machine or a remote machine.
A local worker runs on the same machine as the controller.
A remote worker runs on a different machine.

The `$start_local_workers()` method starts the workers with the `processx` package.
We pass the `n_workers` argument to specify the number of workers.
The `worker_loop` argument is the function that is executed by the workers.
If the `worker_loop` depends on global variables, we can pass them to the `globals` argument.
Packages that are needed by the `worker_loop` can be passed to the `packages` argument.

```{r}
worker_ids = rush$start_local_workers(
  worker_loop = wl_random_search,
  n_workers = 4)
```

We get more information about the workers with `$worker_info`.
The `worker_id` identifies the worker.
The `pid` is the process id of the worker process.

```{r}
Sys.sleep(1)
rush$worker_info
```

The `$worker_states` method returns the state of the workers.
A worker is either `"running"`, `"terminated"`, `"killed"` or `"lost"`.
We see that the workers are running.

```{r}
rush$worker_states
```

We can add more workers to the network.

```{r}
rush$start_local_workers(
  worker_loop = wl_random_search,
  n_workers = 2,
  lgr_thresholds = c(rush = "info"))
```

```{r}
#| include: false
Sys.sleep(1)
```

```{r}
rush$worker_states
```

## Globals

Global are variables that are defined in the global environment but should be available on the workers.
Global variables are passed by name in the `$start_local_workers()`, `start_remote_workers()` and `$worker_script()` methods.
The globals are serialized and stored in the redis data base.
The workers pull the globals from the data base and assign them to their global environment.

## Stop Worker

Workers can be stopped individually or all at once.

```{r}
rush$stop_workers(worker_ids = worker_ids[1])
```

This kills the worker process.

```{r}
Sys.sleep(1)
rush$worker_states
```

Reset workers

```{r}
rush$reset()
```

In the previous example, the optimization was stopped after 100 iterations.
We now want to terminate the optimization with a terminate signal.
For this, we use the `rush$terminated` flag which can be controlled from the `Rush` controller.
We can also terminate the workers by sending a terminate signal.
The difference between killing and terminating a worker is that the worker can catch the terminate signal and finish the current task.
The terminate signal must be implemented in the worker loop via the `rush$terminated` flag.
This flag can be set by the controller.

```{r}
wl_terminate = function(rush) {
  # loop until terminate signal is received
  while(!rush$terminated) {
    # draw new task
    xs = list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15))

    # mark task as running
    key = rush$push_running_tasks(xss = list(xs))

    # evaluate task
    ys = list(y = bbotk::branin(xs$x1, xs$x2))

    # push result
    rush$push_results(key, yss = list(ys))
  }
}
```

```{r}
# Connection to the Redis database
config = redux::redis_config()

# Initialize rush controller
rush = rsh(
  network = "test-random-search",
  config = config)

rush$start_local_workers(
  worker_loop = wl_terminate,
  n_workers = 2,
  lgr_thresholds = c(rush = "info"))
```

The random search runs.

```{r}
Sys.sleep(1)
rush$fetch_finished_tasks()
```

We now want to terminate the optimization.

```{r}
rush$stop_workers(type = "terminate")
```

The workers are termianted.

```{r}
Sys.sleep(1)
rush$worker_states
```

## Failed Workers

```{r}
rush = rsh(network = "test-failed-workers")

wl_failed_worker = function(rush) {
  # simulate a segmentation fault
  tools::pskill(Sys.getpid(), tools::SIGKILL)
}
```

```{r}
worker_ids =  rush$start_local_workers(
  worker_loop = wl_failed_worker,
  n_workers = 2)
```

```{r}
Sys.sleep(1)
rush$worker_states
```

```{r}
rush$detect_lost_workers()
```

## Restart Workers

Workers can be restarted after they failed.

```{r}
rush$restart_workers(worker_ids = worker_ids[1])
```

```{r}
Sys.sleep(1)
rush$worker_states
```

## Log Messages

The worker logs all messages written with the `lgr` package to the database.
The `lgr_thresholds` argument of `$start_workers()` defines the logging level for each logger e.g. `c(rush = "debug")`.
Saving log messages adds a small overhead but is useful for debugging.
By default, no log messages are stored.

```{r}
rush = rsh(network = "test-log-messages")

wl_log_message = function(rush) {
  lg = lgr::get_logger("rush")
  lg$info("This is an info message from worker %s", rush$worker_id)
}
```

```{r}
rush$start_local_workers(
  worker_loop = wl_log_message,
  n_workers = 2,
  lgr_thresholds = c(rush = "info"))
```

Print latest log messages.

```{r}
Sys.sleep(1)
rush$print_log()
```

Read all log files.

```{r}
rush$read_log()
```

Reset Rush Network

```{r}
rush$reset()
```

# Remote Workers

With the `mirai` package it is very easy to start `rush` workers on remote machines
The `mirai` package starts daemons (persistent background processes) that run arbitrary R code in parallel.
The daemons call back to main session.

```{r}
library(mirai)
```

## Start Workers

Start local daemons.

```{r}
daemons(
  n = 2,
  url = host_url()
)

status()
```

Launch daemons on remote machine via SSH.

```{r}
#| eval: false
daemons(
  n = 2L,
  url = host_url(port = 5555),
  remote = ssh_config(remotes = "ssh://10.75.32.90")
)
```

Start daemons on high performance computing clusters.

```{r}
#| eval: false
daemons(
  n = 2L,
  url = host_url(),
  remote = remote_config(
    command = "sbatch",
    args = c("--mem 512", "-n 1", "--wrap", "."),
    rscript = file.path(R.home("bin"), "Rscript"),
    quote = TRUE
  )
)
```

```{r}
wl_random_search = function(rush) {
  repeat {
    # draw new task
    xs = list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15))

    # mark task as running
    key = rush$push_running_tasks(xss = list(xs))

    # evaluate task
    ys = list(y = branin(xs$x1, xs$x2))

    # push result
    rush$push_results(key, yss = list(ys))

    # stop optimization after 100 tasks
    if (rush$n_finished_tasks >= 100) break
  }
}

# Initialize rush controller
rush = rsh(network = "test-remote-worker")
```

After the daemons are started, we can start the remote workers.
The `start_remote_workers()` method starts the workers on `mirai` daemons.

```{r}
daemons(n = 2)

Sys.sleep(1)

worker_ids = rush$start_remote_workers(
  worker_loop = wl_random_search,
  n_workers = 2)
```

```{r}
rush$worker_info
```


## Failed Workers

```{r}
rush = rsh(network = "test-failed-workers")

wl_failed_worker = function(rush) {
  # simulate a segmentation fault
  tools::pskill(Sys.getpid(), tools::SIGKILL)
}
```

```{r}
mirai::daemons(2)

worker_ids = rush$start_remote_workers(
  worker_loop = wl_failed_worker,
  n_workers = 2)
```

```{r}
Sys.sleep(1)
rush$worker_states
```

```{r}
rush$detect_lost_workers()
```

A segfault also tears down the mirai daemon.
Thus, we need to restart the daemon before we can restart the workers.

```{r}
mirai::daemons(2)
mirai::status()
```

Restart the workers.

```{r}
rush$restart_workers(worker_ids)
```

# Script Workers

The most flexible way to start workers is to use a script generate with `$worker_script()`.
The script can be executed on the local machine or a remote machine.
The only requirement is that the machine can run R scripts and has access to the Redis database.

```{r}
rush = rsh(
  network = "test-script-workers",
  config = config)

rush$worker_script(
  worker_loop = wl_random_search)
```

# Error Handling

The heartbeat is a mechanism to monitor the status of script workers in distributed computing systems.
The mechanism consists of a heartbeat key with a set [expiration timeout](https://redis.io/commands/expire/) and a dedicated heartbeat process that refreshes the timeout periodically.
The heartbeat process is started with `callr` and is linked to main process of the worker.
In the event of a worker's failure, the associated heartbeat process also ceases to function, thus halting the renewal of the timeout.
The absence of the heartbeat key acts as an indicator to the controller that the worker is no longer operational.
Consequently, the controller updates the worker's status to `"lost"`.

Heartbeats are initiated upon worker startup by specifying the `heartbeat_period` and `heartbeat_expire` parameters.
The `heartbeat_period` defines the frequency at which the heartbeat process will update the timeout.
The `heartbeat_expire` sets the duration, in seconds, before the heartbeat key expires.
The expiration time should be set to a value greater than the heartbeat period to ensure that the heartbeat process has sufficient time to refresh the timeout.

```{r}
rush$worker_script(
  worker_loop = wl_random_search,
  heartbeat_period = 1,
  heartbeat_expire = 3)
```

The heartbeat process is also the only way to kill a script worker.
The `$stop_workers(type = "kill")` method pushes a kill signal to the heartbeat process.
The heartbeat process terminates the main process of the worker.

