---
title: "Rush Advanced"
vignette: >
  %\VignetteIndexEntry{Rush Adcanved}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

```{r}
library(rush)

wl_random_search = function(rush) {
  repeat {
    # draw new task
    xs = list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15))

    # mark task as running
    key = rush$push_running_tasks(xss = list(xs))

    # evaluate task
    ys = list(y = branin(xs$x1, xs$x2))

    # push result
    rush$push_results(key, yss = list(ys))

    # stop optimization after 100 tasks
    if (rush$n_finished_tasks >= 100) break
  }
}


# Connection to the Redis database
config = redux::redis_config()

# Initialize rush controller
rush = rsh(
  network = "test-random-search",
  config = config)

worker_ids = rush$start_local_workers(
  worker_loop = wl_random_search,
  n_workers = 1)
```

# Retrieve Results {#sec-retrieve-results}

The `$fetch_finished_tasks()` method retrieves the results of finished tasks.
The method returns a `data.table` with additional meta information.
The `worker_id` of the worker that evaluated the task and the `pid` of the worker process are stored in the table.

```{r}
rush$fetch_finished_tasks()
```

There are multiple `$fetch_*()` methods available for retrieving data from the Redis database.
A matching method is defined for each task state e.g. `$fetch_queued_tasks()` and `$fetch_running_tasks()`.
Fetching the results takes around 1 millisecond for one task and 130 milliseconds for 10,000 tasks.
The method `$fetch_finished_tasks()` caches the already queried data (see @sec-caching).

We can change the included columns with the `fields` argument.
The default of `$fetch_finished_tasks()` is `c("xs", "xs_extra", "worker_extra", "ys", "ys_extra")`.
If we don't want all that extra information, we can just query `"xs"` and `"ys"`.

```{r}
rush$fetch_finished_tasks(fields = c("xs", "ys"), reset_cache = TRUE)
```

The option`data_format = "list"` returns a `list` instead of a `data.table`.

```{r}
rush$fetch_finished_tasks(data_format = "list")
```

An advantage of the `data.table` format is that we can easily sort the results and give us access to the best result.
However, when we only need the result, a `list` has less overhead.

## Wait for Results {#sec-wait-for-results}

The `$wait_for_finished_tasks()` methods wait until a new result is available.

```{r}
rush$wait_for_finished_tasks(timeout = 1)
```

## Caching {#sec-caching}

The tasks queried with `$fetch_finished_tasks()` and `$fetch_new_tasks()` are cached.
This gives a significant speedup when we query the same data multiple times.
The cache is a `list` that is stored in the controller.
This takes around 4 milliseconds for 100 tasks, independent of the number of results in the cache (see @sec-benchmark).

```{r}
rush$fetch_finished_tasks()
```

If we change the queried fields, the change is not reflected in the cache.
In this case, we can clear the cache with the `reset_cache = TRUE` option.

## Error Handling {#sec-error-handling}

The `rush` package is equipped with an advanced error-handling mechanism designed to manage and mitigate errors encountered during the execution of tasks.
It adeptly handles a range of error scenarios, from standard R errors to more complex issues such as segmentation faults and network errors.

### Simple R Errors {#sec-error-handling-simple}

For errors occurring within the R environment, rush employs a strategy where such errors are caught during the task evaluation.
To illustrate, consider a scenario where we define a function intended to trigger a simple R error.
The occurring error leads to the task being labeled as `"failed"`, with the corresponding error message captured and stored in the "message" column of the data store.
This process is demonstrated in the following example.

```{r}
#| eval: false
rush = rsh(network_id = "simple_error")

fun = function(x) {
  stop("Simple R Error")
}

rush$start_workers(fun = fun, n_workers = 2)
rush$push_tasks(list(list(x = 1)))

Sys.sleep(5)

rush$fetch_failed_tasks()
```

This approach ensures that errors do not halt the overall execution process, allowing for error inspection and task reevaluation as necessary.

### Handling Failing Workers {#sec-error-handling-workers}

The rush package also addresses scenarios where workers may fail due to crashes or lost connections, potentially causing tasks to appear as if they are in the `"running"` state indefinitely.
An example of this can be seen when simulating a segmentation fault, leading to the termination of a worker process.

If a worker crashes or the connection gets lost, it looks like a task is `"running"` forever.
As an example, we define a function that simulates a segmentation fault by killing the worker process.
The package includes the method `$detect_lost_workers()` designed to identify and manage such instances effectively.

```{r}
#| eval: false
rush = rsh(network_id = "segmenation_fault")

fun = function(x) {
  tools::pskill(Sys.getpid())
}

rush$start_workers(fun = fun, n_workers = 2)
rush$push_tasks(list(list(x = 1)))

Sys.sleep(5)

rush$detect_lost_workers()
```

Running this method periodically adds a small overhead.
For remote workers, the package utilizes the heartbeat mechanism to check the status of the workers (see @sec-heartbeat).
Local workers are monitored using the `processx` package.
Upon identifying a lost worker, its status is updated to `"lost"`.

```{r}
rush$worker_states
```

The `$detect_lost_workers()` method allows to restart lost workers when the option `restart_workers = TRUE` is specified.
Workers that have been lost may also be restarted manually using `$restart_workers()`.
Automatically restarting workers only works for local workers.

The status of the task that caused the worker to fail is changed to `"failed"`.
See @sec-retry-tasks for more information on restarting tasks.

## Large Objects {#sec-large-objects}

The maximum size of a Redis string is 512 MiB.
If the constants of the worker loop function are larger than 512 MiB, `rush` throws an error.
If the controller and workers can access the same file system, `rush` writes the large objects to disk.
The `large_objects_path` argument of `rush_plan()` defines the directory where the large objects are stored.

::: {.callout-note}

In a future version, tasks larger than 512 MiB can be stored on disk with the same mechanism.

:::

# Rush Data Store {#sec-data-store}

Tasks are stored in Redis [hashes](https://redis.io/docs/data-types/hashes/).
Hashes are collections of field-value pairs.
The key of the hash identifies the task in Redis and `rush`.

```
key : xs | ys | xs_extra
```

The field-value pairs are written by different methods, e.g. `$push_tasks()` writes `xs` and `$push_results()` writes `ys`.
The values of the fields are serialized lists or atomic values e.g. unserializing `xs` gives `list(x1 = 1, x2 = 2)`
This data structure allows quick converting of a hash into a row and joining multiple hashes into a table.

```
| key | x1 | x2 | y | timestamp |
| 1.. |  3 |  4 | 7 |  12:04:11 |
| 2.. |  1 |  4 | 5 |  12:04:12 |
| 3.. |  1 |  1 | 2 |  12:04:13 |
```
When the value of a field is a named list, the field can store the cells of multiple columns of the table.
When the value of a field is an atomic value, the field stores a single cell of a column named after the field.
The methods `$push_tasks()` and `$push_results()` write into multiple hashes.
For example, `$push_tasks(xss = list(list(x1 = 1, x2 = 2), list(x1 = 2, x2 = 2))` writes `xs` in two hashes.

::: {.callout-note}

The alternative of writing each cell in a separate field turned out to be too slow because each element has to be serialized individually.
In addition, grouping has the advantage that the worker can simply access the arguments of the function without having to store information about the search space.
The other alternative of writing each row in a string is too slow because serialization and deserialization must be performed each time the row is accessed.

:::
