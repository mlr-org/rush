---
title: "Error Handling"
vignette: >
  %\VignetteIndexEntry{Error Handling}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

# Scope

*rush*  is equipped with an advanced error-handling mechanism designed to manage and mitigate errors encountered during the execution of tasks.
It adeptly handles a range of error scenarios, from standard R errors to more complex issues such as segmentation faults and network errors.
The heartbeat mechanism allows to monitor of remote workers.

# Simple R Errors {#sec-error-handling-simple}

We use the random search example from the [Rush vignette](rush.html) to demonstrate how the error handling mechanism functions.
The user must catch errors within the worker loop and mark the task as `"failed"` using the `$push_failed()` method.

```{r}
library(rush)

wl_random_search = function(rush) {
  repeat {
    # draw new task
    xs = list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15))

    # mark task as running
    key = rush$push_running_tasks(xss = list(xs))

    # try to evaluate task
    tryCatch({
      if (runif(1) < 0.5) stop("Random Error")
      ys = list(y = branin(xs$x1, xs$x2))
      rush$push_results(key, yss = list(ys))
    }, error = function(e) {
      condition = list(message = e$message)
      rush$push_failed(key, conditions = list(condition))
    })

    # push result
    rush$push_results(key, yss = list(ys))

    # stop optimization after 100 tasks
    if (rush$n_finished_tasks >= 20) break
  }
}


# Connection to the Redis database
config = redux::redis_config()

# Initialize rush controller
rush = rsh(
  network = "test-simply-error",
  config = config)

rush$start_local_workers(
  worker_loop = wl_random_search,
  n_workers = 4,
  globals = "branin")
```

The occurring error leads to the task being labeled as `"failed"`, with the corresponding error message captured and stored in the "message" column.
This approach ensures that errors do not halt the overall execution process, allowing for error inspection and task reevaluation as necessary.

```{r}
rush$fetch_failed_tasks()
```

The finished tasks can be retrieved using the `$fetch_finished_tasks()` method.

```{r}
rush$fetch_finished_tasks()
```

# Handling Failing Workers {#sec-error-handling-workers}

The rush package also addresses scenarios where workers may fail due to crashes or lost connections, potentially causing tasks to appear as if they are in the `"running"` state indefinitely.
As an example, we define a function that simulates a segmentation fault by killing the worker process.
The package includes the method `$detect_lost_workers()` designed to identify and manage such instances effectively.

```{r}
#| eval: false
rush = rsh(network = "test-failed-workers")

wl_failed_worker = function(rush) {
  # draw new task
  xs = list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15))

  # mark task as running
  key = rush$push_running_tasks(xss = list(xs))

  # simulate a segmentation fault
  tools::pskill(Sys.getpid(), tools::SIGKILL)
}

worker_ids =  rush$start_local_workers(
  worker_loop = wl_failed_worker,
  n_workers = 2)

Sys.sleep(1)

rush$detect_lost_workers()
```

Upon identifying a lost worker, its status is updated to `"lost"`.

```{r}
rush$worker_states
```

Running this method periodically adds a small overhead.
This method works for workers started with `$start_local_workers()` and `$start_remote_workers()`.
Workers started with `$worker_script()` must be started with a heartbeat mechanism (see @sec-heartbeat).

The `$detect_lost_workers()` method allows to restart lost workers when the option `restart_workers = TRUE` is specified.
Workers that have been lost may also be restarted manually using `$restart_workers()`.
Automatically restarting workers only works for local workers.

The status of the task that caused the worker to fail is changed to `"failed"`.

```{r}
rush$fetch_failed_tasks()
```

# Heartbeat {#sec-heartbeat}

The heartbeat is a mechanism to monitor the status of script workers in distributed computing systems.
The mechanism consists of a heartbeat key with a set [expiration timeout](https://redis.io/commands/expire/) and a dedicated heartbeat process that refreshes the timeout periodically.
The heartbeat process is started with `callr` and is linked to main process of the worker.
In the event of a worker's failure, the associated heartbeat process also ceases to function, thus halting the renewal of the timeout.
The absence of the heartbeat key acts as an indicator to the controller that the worker is no longer operational.
Consequently, the controller updates the worker's status to `"lost"`.

Heartbeats are initiated upon worker startup by specifying the `heartbeat_period` and `heartbeat_expire` parameters.
The `heartbeat_period` defines the frequency at which the heartbeat process will update the timeout.
The `heartbeat_expire` sets the duration, in seconds, before the heartbeat key expires.
The expiration time should be set to a value greater than the heartbeat period to ensure that the heartbeat process has sufficient time to refresh the timeout.

```{r}
rush$worker_script(
  worker_loop = wl_random_search,
  heartbeat_period = 1,
  heartbeat_expire = 3)
```

The heartbeat process is also the only way to kill a script worker.
The `$stop_workers(type = "kill")` method pushes a kill signal to the heartbeat process.
The heartbeat process terminates the main process of the worker.
