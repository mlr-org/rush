---
title: "rush - A Database-Centric Architecture for Distributed Computing"
format: html
bibliography: references.bib
---

<!--
What journal is this for?
- JSS
- JMLR
- Data Mining and Knowledge Discovery
- R Journal
-->

```{r}
#| include: false
library(rush)
library(here)
here::i_am("vignettes/paper.qmd")

r = redux::hiredis()
r$FLUSHDB()
options(datatable.prettyprint.char = 10L)
```

# Abstract {#sec-abstract}

Optimizing modern machine learning algorithms often requires substantial computational resources.
Distributing evaluations across many workers is a common strategy to address this challenge.
However, scaling optimization algorithms to hundreds of workers exposes a fundamental limitation: the central controller becomes a bottleneck.
We present `rush`, an R package for asynchronous and decentralized optimization that removes this single coordination point.
`rush` uses a database-centric architecture in which workers communicate through a shared Redis database and independently run optimization loops.
To support high-throughput workloads, `rush` combines low per-task overhead with caching strategies that reduce database operations.
`rush` integrates with the `mlr3` ecosystem and serves as the backend for asynchronous optimization algorithms in `bbotk` and `mlr3tuning`.
We demonstrate the usability of `rush` by implementing asynchronous decentralized Bayesian optimization.
Further we benchmark the core functionality of `rush` and show that it scales efficiently to over 448 workers while achieving substantially higher CPU utilization than controller-centric approaches.

# Introduction {#sec-introduction}

<!--context-->
Black-box optimization addresses problems where the objective is accessible only through function evaluations and derivatives are unavailable or unreliable.
In machine learning, this setting arises naturally in hyperparameter optimization, where each evaluation corresponds to training and validating a model.
As datasets and model complexity continue to grow, the resulting computational cost often necessitates distributing evaluations across many machines.
But scaling optimization algorithms to hundreds of workers exposes a fundamental limitation: the central controller becomes a bottleneck.
When the central controller cannot generate new tasks fast enough to keep up with the workers, the scalability of the optimization algorithm is limited.

Traditional approaches to parallel computing in R, such as the `parallel` and `future` [@bengtsson2021] packages, follow a controller-worker model where a central process
proposes tasks, dispatches them to workers, and collects results (Figure @fig-rush-network left).
Packages like `batchtools` [@lang2017] and `crew` [@landau2023] address distributed computing on high-performance clusters but follow the same controller-worker model.
The `rrq` package reduces overhead per task by using Redis as a task queue, but still relies on central controller to propose tasks and collect results.
The `mirai` package [@gao2025] emphasizes asynchronous evaluation at scale, but it does not by itself remove controller-centric coordination in optimization workflows.

<!-- research question -->
A decentralized alternative is to let each worker run an optimization loop while exchanging information through shared state, thereby avoiding a single point of coordination (Figure @fig-rush-network right).
Algorithms like Bayesian optimization [@egele2023] and asynchronous hyperband [@li2020] are examples for this.
Software frameworks for decentralized optimization like `Ray Tune` [@liaw2018], `DeepHyper` [@egele2025], `Optuna` [@akiba2019] are available in Python.
In contrast, R lacks a comparable framework for asynchronous and decentralized optimization.

<!-- solution -->
To fill this gap, we present `rush`, a package for decentralized optimization in R.
Unlike existing solutions, `rush`  allows workers to communicate asynchronously over a shared Redis database.
`rush` provides a rich API for task and results and their states
It  features low overhead per task, robust error handling with automatic detection of lost workers, and an efficient caching mechanism that minimizes database operations.
`rush` integrates seamlessly with the mlr3 ecosystem, powering asynchronous optimization in the `bbotk` and `mlr3tuning` packages.

<!-- main conclusion -->
`rush` allows researchers to develop new distributed optimization algorithms and empowers end users to solve large-scale problems with minimal effort.
We demonstrate the usability of `rush` by implementing asynchronous distributed Bayesian optimization.
Our benchmarks show that `rush` scales efficiently to over 448 workers while maintaining high CPU utilization.

::: {#fig-rush-network}

![](network_types.png)

Centralized (left) and decentralized (right) optimization networks.
Hexagons denote processes, including the main process `M` and workers `W`.
`O` denotes the optimizer, which proposes tasks and incorporates their results, and `f` denotes the objective function to be optimized.
In the centralized network, `M` runs `O`, proposes tasks to the workers, and processes the returned evaluations of `f`.
In the decentralized network, each worker `W` runs its own instance of `O`, evaluates `f`, and exchanges task information via a shared `Redis` database (red rectangle).
:::

## Related Work {#sec-related-work}

<!-- Parallel backends -->
As multi-core processors became commonplace in the 2000s, there was a growing need to utilize these resources effectively for computational tasks in R.
The first packages to address this need were `snow` [@tierney2021] and `multicore`.
With R version 2.14.0 (released in 2011), parallel computing capabilities were integrated into the base R system through the `parallel` package.
The functions `parallel::mclapply()` and `parallel::parLapply()` are parallel versions of the `lapply()` function for multicore and cluster computing, respectively.
Both functions are widely used in R packages but have some limitations.
The R session is blocked until all tasks are finished and it is not possible to retrieve partial results.
Moreover, load balancing can be an issue when the tasks have different runtimes.

The landscape further evolved with the release of the `future` [@bengtsson2021] package in 2016, which provided a unified and flexible parallel computing interface in R, supporting various backends such as `multisession`, `multicore`, and `callr`.
The `multisession` and `cluster` backends are limited to 125 workers.
Since each future is by default evaluated in a new process, the overhead per task can be significant.
The `future.apply` package implements parallel versions of the `*apply()` family functions, compatible with the `future` backends.

The `mirai` package [@gao2025] evaluates an R expression asynchronously in a parallel process, locally or distributed over the network.
The package has a strong focus on throughput, low overhead and modern networking, eliminating many drawback of the older approaches.

<!-- Distributed computing and HPC -->
With the rise of high-performance computing (HPC) clusters, the `batchtools` [@lang2017] package was developed to facilitate the execution of long-running tasks on these systems.
The communication between the main process and the workers runs completely over the file system.
A notable feature of the package is the assistance in conducting large-scale computer experiments.
A more recent development in distributed computing is the `crew` [@landau2023] package.
The package is designed for long-running tasks in distributed systems, ranging from traditional high-performance clusters to cloud computing platforms.
A drawback of both systems is the high overhead per task.

<!-- Python -->
The Python ecosystem offers several tools for distributed computing in machine learning.
DeepHyper [@egele2025] supports massively parallel hyperparameter optimization workflows.
Ray Tune [@liaw2018] provides distributed hyperparameter optimization and experiment management.
Dask [@rocklin2015] enables parallel and distributed execution via a dynamic task scheduler that scales from a single machine to multi-node clusters.

<!-- Database connectivity layers -->
The parallel and distributed computing ecosystem in R is complemented by database connectivity layers that enable shared state and coordination.
For relational database management systems, `DBI` [@rsigdb2024] defines a standardized interface that separates a common front end from database-specific back ends.
For `Redis`, `redux` provides R bindings to the Redis API and exposes the command set through a low-level and a higher-level interface.

The `rrq` package is a task queue system for R using Redis as a coordination backend.
It addresses the limitations of other packages by providing a non-blocking interface to parallel computing and keeping the overhead per task low.
The package allows non-interacting queues with priority levels within a queue and dependencies among tasks.
`rrq` is implemented on top of `redux`, which provides the underlying Redis connectivity and command interface.
The package has an advanced error-handling mechanism, heavily influencing the heartbeat mechanism of `rush`.

# General Structure of Rush {#sec-general-structure}

A Rush network consists of multiple workers that communicate via a shared Redis database.
Each worker evaluates tasks and pushes the corresponding results back to the database.

::: {#fig-rush-network}

![](rush_network.png){width=100%}

The architecture of a rush network.
A hexagon represents a worker and the rectangle represent the `Redis` database.
Each worker `W` runs its own instance of `O`, evaluates `f`, and exchanges task information via a shared `Redis` database.
The optimizer `O` gathers information about the tasks and their results with `$fetch_finished_tasks()`.
Creates a new task based on these information and store it in the `Redis` database with `$push_running_tasks()`.
When the task is evaluated, the results are pushed to the `Redis` database with `$push_results()`.
:::

We illustrate the basic functionality of Rush using an example optimization problem based on the Branin function.

$$f(x_1,x_2)=\left(x_2-\frac{5.1}{4\pi^2}x_1^2+\frac{5}{\pi}x_1-6\right)^2 +10\left(1-\frac{1}{8\pi}\right)\cos(x_1)+10$$

The function is optimized over the domain $x_1 \in [-5, 10]$ and $x_2 \in [0, 15]$, as shown in Figure @fig-branin.
The Branin function serves as a toy benchmark for optimization that is fast to evaluate yet sufficiently nontrivial.

![Branin function](branin.png){#fig-branin width=100%}


```{r}
branin <- function(x1, x2) {
  (x2 - 5.1 / (4 * pi^2) * x1^2 + 5 / pi * x1 - 6)^2 +
  10 * (1 - 1 / (8 * pi)) * cos(x1) +
  10
}
```



## Worker Loop

We define the `worker_loop` function, which is executed on each worker.
The function repeatedly creates tasks, evaluates them, and sends the results to the Redis database.
It takes a single argument, a `RushWorker` object, which manages communication with Redis.
In this example, each worker samples a random point, constructs a task, evaluates it using the Branin function, and submits the result.
The optimization terminates after 100 tasks have been evaluated.

```{r rush-003}
library(rush)

wl_random_search = function(rush) {

  while(rush$n_finished_tasks < 100) {

    xs = list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15))
    key = rush$push_running_tasks(xss = list(xs))

    ys = list(y = branin(xs$x1, xs$x2))
    rush$push_results(key, yss = list(ys))
  }
}
```

The most important methods of the `RushWorker` class are `$push_running_tasks()` and `$push_results()`.
The `$push_running_tasks()` method creates a new task in the Redis database.
Because the task is scheduled for immediate evaluation, it is marked as running.
The `$push_running_tasks()` method returns a unique key that is used to identify the task.

The `$push_results()` method pushes the corresponding results back to the Redis database.
It takes the task key and a list of result values as arguments.

Marking a task as running is not essential for random search.
However, it is crucial for more sophisticated algorithms that use information about other workers’ tasks to decide which task to evaluate next.
For example, Bayesian optimization algorithms select new evaluation points that are farther from previously sampled points in order to explore the search space more effectively.
The `$n_finished_tasks` field indicates how many tasks have been completed and is used to terminate the worker loop.

## Tasks

Tasks are the basic units through which workers exchange information.
The main components of a task are its key, computational state, input (`xs`), and output (`ys`).
The key is a unique identifier for the task.
It is used to reference the task in the Redis database.

The four possible computational states are `"running"`, `"finished"`, `"failed"`, and `"queued"`.
The `$push_running_tasks()` method marks a task as `"running"` and returns its key.
The `$push_results()` method marks a task as `"finished"` and stores the associated result.
Failed tasks can be marked as `"failed"` using the `$push_failed()` method.
Error handling must be implemented explicitly in the worker loop (see @sec-error-handling for details).
Tasks can also be pushed to a queue using the `$push_tasks()` method, which sets their state to `"queued"`.

The input `xs` and output `ys` are lists that may contain arbitrary data.
The methods of the `RushWorker` can operate on multiple tasks simultaneously.
For this reason, `xxs` and `yss` denote lists of inputs and outputs, respectively.

## Manager

The `rush` manager is responsible for starting, monitoring, and stopping workers within the network.
It is initialized using the `rsh()` function, which requires a network identifier and a `config` argument.
The `config` argument specifies a configuration file used to connect to the Redis database via the `redux` package.

```{r rush-004}
library(rush)

config = redux::redis_config()

rush = rsh(
  network = "test-random-search",
  config = config)
```

Workers can be started using the `$start_local_workers()` method, which accepts the worker loop and the number of workers as arguments.
By default, workers are started locally using the `processx` package.

```{r rush-005}
rush$start_local_workers(
  worker_loop = wl_random_search,
  branin = branin,
  n_workers = 4)

rush
```

The optimization completes quickly and the results can then be retrieved.
The `$fetch_finished_tasks()` method fetches all finished tasks from the database.
It returns a `data.table()` containing the task key, input, and result.
The `pid` and `worker_id` columns store additional information recorded when each task is created.
The `worker_id` identifies the worker that evaluated the task, and the `pid` corresponds to the process identifier of that worker.
Further auxiliary information can be passed as `list`s to the `$push_running_tasks()` and `$push_results()` methods via the `extra` argument.

```{r rush-006}
#| include: false
Sys.sleep(5)
```

```{r rush-007}
#rush$fetch_finished_tasks()[order(y)]
```

The `rush` controller displays the number of running workers and the number of tasks in each state.
In this example, 103 tasks are marked as finished and all workers have stopped.
This number slightly exceeds 100 because workers check the stopping condition independently.
If several workers evaluate the condition at approximately the same time, for example when 99 tasks are finished, they may all create new tasks before detecting that the limit has been reached.
Additionally, tasks may continue to be created while the 100th task is still being evaluated.

```{r rush-008}
rush
```

The workers can be stopped and the database reset using the `$reset()` method.

```{r rush-009}
rush$reset()

rush
```


## Data Storage

Rush stores all data in a Redis database.
Redis is an open-source, in-memory key-value store commonly used as a database, cache, and message broker.
It is valued for extremely low latency and supports multiple data structures such as strings, hashes, lists and sets.
Rush uses different data structures of Redis to store the data.

Redis hashes are a data structure for storing associative field–value pairs under a single key.
They are optimized for memory efficiency and fast access to structured data.
This makes them suitable for representing the input, output and meta information of tasks.
The key of the hash identifies the task in Redis and rush.

```
key : xs | ys | xs_extra
```

The field-value pairs are written by different methods, e.g. `$push_running_tasks()` writes `xs` and `$push_results()` writes `ys`.
The values of the fields are serialized lists or atomic values e.g. unserializing `xs` gives `list(x1 = 1, x2 = 2)`
This data structure allows quick converting of a hash into a row and joining multiple hashes into a table.

```
| key | x1 | x2 | y | timestamp |
| 1.. |  3 |  4 | 7 |  12:04:11 |
| 2.. |  1 |  4 | 5 |  12:04:12 |
| 3.. |  1 |  1 | 2 |  12:04:13 |
```

Redis sets are an unordered collection of unique elements associated with a single key.
They enforce uniqueness automatically and support efficient membership tests.
Sets are used to store the state of tasks.
For example, all running tasks are stored in the `"running_tasks"` set.

Redis lists are ordered collections of elements stored as linked lists under a single key.
They support efficient insertion and removal at both the head and the tail, enabling queues and event logs where ordering is significant.
Finished and queued tasks are stored in a list.
Using lists for the queue is an obvious choice because it allows to push and pop tasks from the head and the tail.
Storing the finished tasks in a list gives them an order by time.
This allows to cheaply get the latest results and cache the finished tasks.

# Advanced Features of Rush

## Queues

Sometimes it is advantageous to create tasks in central process and push them to the workers.
This avoids to coordinate which worker evaluates which task.

The queue system works by pushing and popping tasks from a queue.
The `$push_task()` method creates new tasks and pushes them to the queue.
In this example, we create 25 tasks and push them to the queue.

```{r rush-016}
xss = replicate(25, list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15)), simplify = FALSE)

rush$push_tasks(xss = xss)

rush
```

We see 25 queued tasks in the database.
To retrieve the tasks from the queue, we need to implement the `$pop_task()` method in the worker loop.

```{r rush-017}
wl_queue = function(rush) {
  repeat {
    task = rush$pop_task()
    if (is.null(task)) break
    ys = list(y = branin(task$xs$x1, task$xs$x2))
    rush$push_results(task$key, yss = list(ys))
  }
}
```

The `$pop_task()` method pops a task from the queue and marks it as running.
It returns the task or `NULL` if the queue is empty.
We use this to stop the worker loop when the queue is empty.

## Error Handling {#sec-error-handling}

rush is equipped with an advanced error-handling mechanism designed to manage and mitigate errors encountered during the execution of tasks.
It adeptly handles a range of error scenarios, from standard R errors to more complex issues such as segmentation faults and network errors.

Within the worker loop, users are responsible for catching errors and marking the corresponding task as `"failed"` using the `$push_failed()` method.

```{r rush-018}
wl_error = function(rush) {

  while(rush$n_finished_tasks < 100) {

    xs = list(x1 = runif(1, -5, 10), x2 = runif(1, 0, 15))
    key = rush$push_running_tasks(xss = list(xs))

    tryCatch({
      ys = list(y = branin(xs$x1, xs$x2))
      rush$push_results(key, yss = list(ys))
    }, error = function(e) {
      condition = list(message = e$message)
      rush$push_failed(key, conditions = list(condition))
    })
  }
}
```

The rush package provides mechanisms to address situations in which workers fail due to crashes or lost connections.
Such failures may result in tasks remaining in the "running" state indefinitely.
The package offers the `$detect_lost_workers()` method, which is designed to identify and manage these occurrences.
The controller updates the state of the worker to `"lost"`.

This method works for workers started with `$start_local_workers()` and `$start_remote_workers()`.
Workers started with `$worker_script()` must be started with a heartbeat mechanism
The mechanism consists of a heartbeat key with a set expiration timeout and a dedicated heartbeat process that refreshes the timeout periodically.
The heartbeat process is started with `callr` and is linked to the main process of the worker.
In the event of a worker's failure, the associated heartbeat process also ceases to function, thus halting the renewal of the timeout.
The absence of the heartbeat key acts as an indicator to the controller that the worker is no longer operational.
Consequently, the controller updates the worker's status to `"lost"`.

Heartbeats are initiated upon worker startup by specifying the `heartbeat_period` and `heartbeat_expire` parameters.
The `heartbeat_period` defines the frequency at which the heartbeat process will update the timeout.
The `heartbeat_expire` sets the duration, in seconds, before the heartbeat key expires.
The expiration time should be set to a value greater than the heartbeat period to ensure that the heartbeat process has sufficient time to refresh the timeout.

```{r controller-054}
rush$worker_script(
  worker_loop = wl_random_search,
  heartbeat_period = 1,
  heartbeat_expire = 3)
```

The heartbeat process is also the only way to kill a script worker.
The `$stop_workers(type = "kill")` method pushes a kill signal to the heartbeat process.
The heartbeat process terminates the main process of the worker.

Additionally, the Redis database can be saved to disk periodically.

## Log Messages

Workers record all messages generated using the `lgr` package to the database.
The `lgr_thresholds` argument of `$start_local_workers()` specifies the logging level for each logger, e.g. `c("mlr3/rush" = "debug")`.
While enabling log message storage introduces a minor performance overhead, it is valuable for debugging purposes.
By default, log messages are not stored.

## Debugging

* If all of this fails, the user can manually debug the worker loop.
* Output and message logs can be written to files by specifying the `message_log` and `output_log` arguments.


# Tutorial

We implement Asynchronous Distributed Bayesian Optimization (ADBO) [@egele2023] next.
This example shows how workers use information about running and finished tasks and introduces task queues.
ADBO runs sequential [Bayesian optimization](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-bayesian-optimization) on multiple workers in parallel.
Each worker maintains its own surrogate model (a random forest) and selects the next hyperparameter configuration by maximizing the upper confidence bounds acquisition function.
To promote a vathe optimization of algorithmsrying exploration-exploitation tradeoff between the workers, the acquisition functions are initialized with different lambda values ranging from 0.1 to 10.
When a worker completes an evaluation, it asynchronously sends the result to its peers via a Redis data base; each worker then updates its local model with this shared information.
This decentralized design enables workers to proceed independently; eliminating the need for a central coordinator that could become a bottleneck in large-scale optimization scenarios.

# Benchmarks

## Bayesian Optimization

### Experimental Setup

We compare the effective cpu utilization of BO algorithms on hyperparameter optimization tasks.
Each algorithm applies a different approach to parallelize the BO loop.
We optimize nine hyperparameters of the LightGBM learner on four different datasets, as summarized in Tables @tab-hyperparameters and @tab-datasets.
The datasets differ both in the number of features and the number of observations to induce different runtimes of the training process.
This setup results in 12 experiments, each running for 10 minutes on 448 workers.

Sequential BO with multi-point batch proposals evaluates $q$ hyperparameter configurations in parallel.
To obtain a batch of $q$ configurations the constant liar (CL) [@ginsbourger2010] strategy is used.
Here, the first point is obtained by ordinary maximization of the acquisition function.
We then preliminarily impute a fake value, update the surrogate model and propose the next point by maximizing the acquisition function again.
Common choices for the imputed value are the minimum or maximum of the already evaluated points or the predicted posterior mean.
`mlr3` makes use of the `mirai` package [@gao2025] to parallelize over the resampling iterations.
In our setup, CL proposes 44 hyperparameter configurations that are evaluated with 10-fold cross-validation, resulting in 440 parallel evaluations.

The batch parallelization requires all workers to finish before the next batch can be proposed.
If there are strongly heterogeneous runtimes of the training process, the optimizer may end up waiting for the last model to finish while the other workers are idle.
This is a synchronization overhead.
We intentionally introduce heterogeneous runtimes by using early stopping to find the optimal number of boosting iterations.
Some models may train only for a few iterations while others may train for up to 5000 iterations.

Asynchronous Centralized Bayesian Optimization (ACBO) eliminates the synchronization overhead by evaluating the points asynchronously.
When a worker becomes idle, the main process proposes a new configuration.
The configuration is send to the worker without waiting for other evaluations to complete.
To ensure diversity among proposals, configurations currently being evaluated are imputed, akin to the constant liar approach.

However, the fitting of the surrogate model and the optimization of the acquisition function remain centralized.
These steps are executed sequentially by the main process.
With many workers, the main process becomes a bottleneck.
New points cannot be proposed fast enough to keep up with the workers.

Asynchronous Decentralized Bayesian Optimization (ADBO) [@egele2023] distributes the surrogate updates and the acquisition function optimization across the workers.
Each worker maintains its own surrogate model and proposes the next hyperparameter configuration by maximizing the acquisition function.
When a worker completes an evaluation, it asynchronously sends the result to its peers; each worker then updates its local model with this shared information.
Like CL and ACBO, configurations currently being evaluated are imputed.
ADBO also uses stochastic acquisition functions to promote a varying exploration-exploitation tradeoff between the workers.
For example, the lambda values of the upper confidence bounds acquisition function are initialized with different values for each worker.
For algorithms like ADBO, a system like rush is necessary, allowing the workers to asynchronously exchange information.
With rush, the the main process can be avoided completely.

The benchmarks are focused on the effective CPU utilization of the algorithms following the recommendations of @egele2023.
The effective CPU utilization is an important metric because more utilization implies more evaluated points per time unit.
Increased evaluation throughput might lead to a higher final performance.
Let $n_{workers}$ denote the number of workers, where each worker corresponds to a single CPU core
Let $T_{walltime}$ be the elapsed real time from the start to the end of the optimization.
The total consumed CPU time is $T_{CPU} = T_{walltime} \cdot n_{workers}$.
Let $T_{optimization}$ denote the cumulative time spent on surrogate fitting, acquisition optimization, and model training.
The effective CPU utilization is defined as $U = \frac{T_{optimization}}{T_{CPU}}$.

The benchmark was conducted on linux cluster at the Leibniz Supercomputing Centre (LRZ).
Each node has 112 Intel Xeon Sapphire Rapids cores and 488 GiB of memory.
The cluster runs the operating system SUSE Linux Enterprise Server 15 SP6.
All code is available on GitHub at https://github.com/slds-lmu/paper_2026_rush.


### Results

::: {#tbl-benchmarks}

| Task               | Algorithm | Mean Runtime CV [s] | Evaluations | Utilization [%] |
|:-------------------|:----------|--------------------:|------------:|----------------:|
| german-credit      | CL        |                   1 |         396 |            0.33 |
|                    | ACBO      |                   0 |         461 |            0.31 |
|                    | ADBO      |                   1 |      15,451 |           95.00 |
| kddcup09-appetency | CL        |                  27 |         132 |            1.10 |
|                    | ACBO      |                  23 |         473 |            4.50 |
|                    | ADBO      |                  30 |       6,702 |           94.00 |
| adult              | CL        |                  11 |         308 |            1.60 |
|                    | ACBO      |                  12 |         463 |            2.30 |
|                    | ADBO      |                  14 |      10,169 |           96.00 |
| airlines           | CL        |                 313 |          88 |            9.70 |
|                    | ACBO      |                 183 |         435 |           31.00 |
|                    | ADBO      |                 261 |         993 |          100.00 |

Effective CPU utilization and the number of completed evaluations for CL, ACBO, and ADBO across four benchmark tasks.
For each task–algorithm combination, the table reports the mean runtime of the 10-fold cross-validation in seconds, the total number of completed evaluations, and the resulting effective CPU utilization in percent.
The CPU utilization increases with the mean runtime of the 10-fold cross-validation for all algorithms.
:::

The effective CPU utilization and the number of completed evaluations are summarized in Table @tbl-benchmarks.
Across all tasks, ADBO achieves substantially higher utilization than the alternative algorithms.
The improvement is consistent across datasets with both short and long training runtimes.
Higher utilization directly translates into a larger number of completed evaluations.
As a consequence, ADBO explores the search space far more extensively within the same wall-clock budget.

On tasks with very short learner runtimes, centralized methods leave most compute resources idle.
This effect is particularly visible on the german-credit task.
Here, CL and ACBO complete only a few hundred evaluations.
In contrast, ADBO completes more than an order of magnitude additional evaluations.
The utilization gap exceeds two orders of magnitude in this regime.

On medium-cost tasks such as adult and kddcup09-appetency, the same pattern persists.
ACBO improves utilization relative to CL by reducing synchronization overhead.
Nevertheless, the centralized update step remains a limiting factor.

The airlines task has the longest learner training time.
Longer evaluations reduce the relative overhead of model updates.
As a result, utilization increases for all algorithms.
Even in this favorable regime, ADBO maintains a substantial advantage.
Its utilization remains at least three times higher than that of ACBO.
The decentralized design therefore scales robustly across workload regimes.
See Figure @fig-cl-cpu-utilization for an illustration of the CPU utilization of CL under short-running and long-running learner runtimes.

The wall-clock runtime of CL frequently exceeds the nominal 10-minute budget.
This effect is documented in Table @tbl-benchmarks-extra.
CL checks the termination criterion only after an entire batch finishes.
If a batch starts shortly before the time limit, the experiment continues until all evaluations complete.
This behavior introduces systematic budget overruns.
ACBO and ADBO apply termination checks after every completed evaluation.
This finer-grained control enforces the wall-clock constraint more accurately.
Asynchronous scheduling therefore improves both utilization and budget adherence.

::: {#fig-cl-cpu-utilization}

![](cl_cpu_utilization.png)

CPU utilization over time for CL under short-running (left) and long-running (right) objective functions.
Red rectangles denote the proposal phase, and blue rectangles denote the evaluation phase.
Rectangle height indicates CPU utilization relative to the 10-core machine.
During proposal, CL uses a single core (≈10% utilization), whereas during evaluation it uses all 10 cores (≈100% utilization).
The proposal phase becomes longer over iterations because fitting the surrogate model becomes more expensive as the training set grows.
For long-running objectives, evaluation dominates wall-clock time, yielding higher average CPU utilization than for short-running objectives.
:::

## Rush

## Experimental Setup

The evaluation of short-running tasks is sensible to the overhead introduced by rush.
Therefore, we measure the runtime of the core functions in a synthetic benchmark.
This includes `$push_running_tasks()`, `$push_results()`, and `$fetch_finished_tasks()`.
The fetching operation is done with and without caching.
Runtime is reported as median and median absolute deviation (MAD) over 1000 replicates.
The runtime is measured with the `microbenchmark` R package.

## Results

::: {#tbl-runtime-results}

| Operation                              | Mean Runtime [ms] | MAD [ms] |
|:---------------------------------------|:------------------|:---------|
| `$push_running_tasks()`                | 0.5               | 0.25     |
| `$push_results()`                      | 0.5               | 0.25     |

Runtime of the core functions of rush.
The runtime is reported as median and median absolute deviation (MAD) over 1000 replicates.
:::

The runtime of the core functions of rush is summarized in Table @tbl-runtime-results.
Pushing a running task to the database is the most frequent operation in rush.
This operation takes 0.5 ms (MAD 0.25 ms) on average.
Pushing a result to the database takes around the same time.
The workers usually fetch new results frequently, so fetching has a build in caching mechanism.
The difference between fetching with and without caching is negligible for a small number of tasks.
However, fetching with caching is much faster when caching more than 1000 tasks, as shown in Figure @fig-fetch-finished-tasks.
The difference grows with the number of tasks fetched.
We fetch one new tasks from the database and the rest from the cache.
Still the runtime increases with the number of tasks fetched.
This is because binding a new row to a `data.table` gets slower with the total number of rows.

```{r}
#| echo: false
#| fig-cap: |
#|   Runtime of fetching finished tasks with (red lines) and without (blue lines) caching.
#|   The x-axis shows the number of tasks fetched from the database.
#|   The fetching with caching gets one new task from the database and the rest from the cache.
#|   The fetching without caching gets all tasks from the database.
#| label: fig-fetch-finished-tasks
library(data.table)
library(ggplot2)

gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

data = fread(here("vignettes/rush_fetch_finished_tasks.csv"))
data_cache = fread(here("vignettes/rush_fetch_cached_tasks.csv"))

data = rbindlist(list(
  data[, list(n_tasks = n_tasks, runtime = median_runtime, benchmark = "fetch_tasks")],
  data_cache[, list(n_tasks = cache_size, runtime = median_runtime, benchmark = "fetch_cached_tasks")]
))

ggplot(data, aes(x = n_tasks, y = runtime, color = benchmark)) +
  scale_x_continuous(trans='log10') +
  #scale_y_continuous(trans='log10') +
  geom_line() +
  geom_point() +
  scale_color_manual(values = gg_color_hue(2), labels = c("with cache", "without cache")) +
  labs(x = "Number of Tasks", y = "Runtime (ms)", color = "Fetching") +
  theme_minimal()
```


# Discussion and Conclusion

We presented rush, a novel package for distributed computing in R.

# Acknowledgments

The authors gratefully acknowledge the computational and data resources provided by the Leibniz Supercomputing Centre (www.lrz.de).

# Appendix

## Glossary

* Parallel computing: The use of multiple processing elements simultaneously for solving a computational problem.
* Distributed computing: Utilizing multiple computers in a network to solve a computational problem.
* Worker: A process that performs tasks as part of a larger computation.
* Computing task: A discrete portion of a larger computational problem, designed to be executed by a worker.
* Redis: An open-source, in-memory data store, used as a database and for inter-process communication.
* Inter-process communication: Set of mechanisms that allow separate processes to exchange data and coordinate their execution.

## Performance of the BO algorithms

![Performance of the three algorithms](performance.png){#fig-performance-algorithms}

## Runtime of the BO algorithms

::: {#tbl-benchmarks-extra}

|Task               |Algorithm | Runtime Learners| Runtime Surrogate| Runtime Optimizer| Mean Runtime Learners [s]| Walltime [s]| CPU Hours| Performance|
|:------------------|:---------|----------------:|-----------------:|-----------------:|-------------------------:|------------:|---------:|-----------:|
|german-credit      |CL      |              298|               164|               365|                         1|          563|        70|       0.200|
|                   |acbo      |              227|               209|               386|                         0|          595|        74|       0.190|
|                   |adbo      |           11,077|           214,301|            27,884|                         1|          596|        74|       0.180|
|kddcup09-appetency |CL      |            3,625|                27|               100|                        27|          752|        94|       0.018|
|                   |acbo      |           11,088|               205|               374|                        23|          580|        72|       0.018|
|                   |adbo      |          198,362|            37,236|             8,804|                        30|          582|        72|       0.017|
|adult              |CL      |            3,512|               106|               267|                        11|          554|        69|       0.120|
|                   |acbo      |            5,474|               214|               381|                        12|          593|        74|       0.120|
|                   |adbo      |          139,828|           101,105|            14,214|                        14|          596|        74|       0.120|
|airlines           |CL      |           27,517|                18|                74|                       313|          637|        79|       0.320|
|                   |acbo      |           79,643|               196|               385|                       183|          580|        72|       0.320|
|                   |adbo      |          259,320|               384|               648|                       261|          584|        73|       0.320|
:::
